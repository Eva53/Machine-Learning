---
title: "Decision Tree"
author: "Ефременко Влада"
date: "2023-11-09"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Задание 1

Загрузите набор данных Glass из пакета “mlbench”. Набор данных (признаки, классы) был изучен в работе «Метод ближайших соседей». Постройте дерево классификации для модели, задаваемой следующей формулой: Type ~ ., дайте интерпретацию полученным результатам. При рисовании дерева используйте параметр cex=0.7 для уменьшения размера текста на рисунке, например, text(bc.tr,cex=0.7) или draw.tree(bc.tr,cex=0.7). Является ли построенное дерево избыточным? Выполните все операции оптимизации дерева.

```{r Glass}
data("Glass")
Glass
```

```{r Glass}
model <- tree(Type ~ ., Glass)
draw.tree(model, cex=0.5)
```
## Интерпретация полученных результатов:
Из визуализации дерева решений видно, что оно является избыточным, так как есть разветвления, где выбор происходит между одинаковыми классами, например, 19-20.

```{r Glass}
model
```
Выполним оптимизацию дерева:

```{r Glass}
opt_model <- snip.tree(model, nodes = c(26, 108, 31))
draw.tree(opt_model, cex=0.5)
```

```{r Glass}
pruned_model <- prune.tree(opt_model, k=10)
draw.tree(pruned_model, cex=0.7)
```
Удалили избыточные ветви.

# Задание 2

Загрузите набор данных spam7 из пакета DAAG. Постройте дерево классификации для модели, задаваемой следующей формулой: yesno ~., дайте интерпретацию полученным результатам. Запустите процедуру “cost-complexity prunning” с выбором параметра k по умолчанию, method = ’misclass’, выведите полученную последовательность деревьев. Какое из полученных деревьев, на Ваш взгляд, является оптимальным? Объясните свой выбор.

```{r spam}
data(spam7)
spam7
```

```{r spam}
model <- tree(yesno ~ ., spam7)
draw.tree(model, cex=0.7)
```
## Интерпретация полученных результатов:
Из визуализации дерева решений видно, что оно является избыточным, так как есть разветвления, где выбор происходит между одинаковыми классами, например, 3,4,5.
Запустим процедуру “cost-complexity prunning”:

```{r Glass}
pruned_model <- prune.misclass(model)
plot(pruned_model)
pruned_model

for(k in c(0.0, 4.5, 137.5)){
  draw.tree(prune.tree(model, k = k, method = 'misclass'), cex=0.7)
}
```
## Какое из полученных деревьев является оптимальным?
Дерево, полученное после выполнения процедуры с параметром k=0 уже выглядит оптимальным, так как в нем нет избыточности, а также сохранены значимые разветвления.

# Задание 3

Загрузите набор данных nsw74psid1 из пакета DAAG. Постройте регрессионное дерево для модели, задаваемой следующей формулой: re78 ~.. Постройте регрессионную модель и SVM-регрессию для данной формулы. Сравните качество построенных моделей, выберите оптимальную модель и объясните свой выбор.

```{r nsw}
data(nsw74psid1)
nsw74psid1
```
```{r nsw}
n <- dim(nsw74psid1)[1]

nsw74psid1_rand <- nsw74psid1[order(runif(n)),]
df_train <- nsw74psid1_rand[1:as.integer(n*0.8),]
df_test <- nsw74psid1_rand[(as.integer(n*0.8)+1):n,]
```


```{r nsw}
model_tree <- tree(re78 ~., df_train)
model_svm <- svm(df_train[-10], df_train$re78, type="eps-regression", cost=1, eps=0.25)
draw.tree(model_tree, cex=0.7)
```

```{r nsw}
predict_tree <- predict(model_tree, df_test[-10])
predict_svm <- predict(model_svm, df_test[-10])

mist_tree <- sd(df_test$re78 - predict_tree)
mist_svm <- sd(df_test$re78 - predict_svm)

print("Ошибка на тестовых данных, полученная деревом решений: ")
mist_tree

print("Ошибка на тестовых данных, полученная SVM: ")
mist_svm
```
Jшибка на тестовой выборке c использованием модели SVM меньше по сравнению с деревом решений, поэтому она оптимальнее.

# Задание 4

Загрузите набор данных Lenses Data Set из файла Lenses.txt: 
3 класса (последний столбец): 1 : пациенту следует носить жесткие контактные линзы, 2 : пациенту следует носить мягкие контактные линзы, 3 : пациенту не следует носить контактные линзы. 
Признаки (категориальные): 
1. возраст пациента: (1) молодой, (2) предстарческая дальнозоркость, (3) старческая дальнозоркость 
2. состояние зрения: (1) близорукий, (2) дальнозоркий 
3. астигматизм: (1) нет, (2) да 
4. состояние слезы: (1) сокращенная, (2) нормальная
Постройте дерево решений. Какие линзы надо носить при предстарческой дальнозоркости, близорукости, при наличии астигматизма и сокращенной слезы?

```{r lenses}
lenses <- read.table("Lenses.txt")
lenses <- lenses[, -1]
lenses$V6 <- as.factor(lenses$V6)
lenses
```

```{r lenses}
model <- tree(V6 ~., lenses)
draw.tree(model, cex=0.7)
```
```{r lenses}
data_test <- data.frame(V2=2, V3=1, V4=2, V5=1)
prediction <- predict(model, data_test)
cat("Предсказанный класс:", prediction,"\n")
```

# Задание 5

Постройте дерево решений для обучающего множества Glass, данные которого характеризуются 10-ю признаками: 
  1. Id number: 1 to 214; 2. RI: показатель преломления; 3. Na: сода (процент содержания в соотвествующем оксиде); 4. Mg; 5. Al; 6. Si; 7. K; 8. Ca; 9. Ba; 10. Fe.
Классы характеризуют тип стекла:
  (1) окна зданий, плавильная обработка
  (2) окна зданий, не плавильная обработка
  (3) автомобильные окна, плавильная обработка
  (4) автомобильные окна, не плавильная обработка (нет в базе)
  (5) контейнеры
  (6) посуда
  (7) фары
Посмотрите заголовки признаков и классов. Перед построением классификатора необходимо также удалить первый признак Id number, который не несет никакой информационной нагрузки. Это выполняется командой glass <- glass[,-1].
Определите, к какому типу стекла относится экземпляр с характеристиками 
RI =1.516 Na =11.7 Mg =1.01 Al =1.19 Si =72.59 K=0.43 Ca =11.44 Ba =0.02 Fe =0.1 

```{r Glass}
data("Glass")
Glass
```
```{r Glass}
model <- tree(Type ~ ., Glass)
draw.tree(model, cex=0.5)
```
```{r Glass}
data_test <- data.frame(RI=1.516, Na =11.7, Mg =1.01, Al =1.19, Si =72.59, K=0.43, Ca =11.44, Ba =0.02, Fe =0.1)
prediction <- predict(model, data_test)
cat("Предсказанный класс:\n")

prediction
```
С вероятностью 0.875 экземпляр относится к классу (2) окна зданий, не плавильная обработка.

# Задание 6

Для построения классификатора используйте заранее сгенерированные обучающие и тестовые выборки, хранящиеся в файлах svmdata4.txt, svmdata4test.txt.

```{r task6}
train_data <- read.table("svmdata4.txt",stringsAsFactors = TRUE)
test_data <- read.table("svmdata4test.txt",stringsAsFactors = TRUE)
```

```{r task6}
# График для обучающей выборки
plot_train <- ggplot(train_data, aes(x = X1, y = X2, color = factor(Colors))) +
  geom_point(size = 1) +
  labs(x = "X1", y = "X2", title = "Train Data") +
  scale_color_manual(values = c("red" = "red", "green" = "green")) +
  theme_minimal() +
  theme(aspect.ratio = 1, legend.position = "none")

# График для тестовой выборки
plot_test <- ggplot(test_data, aes(x = X1, y = X2, color = factor(Colors))) +
  geom_point(size = 1) +
  labs(x = "X1", y = "X2", title = "Test Data") +
  scale_color_manual(values = c("red" = "red", "green" = "green")) + 
  theme_minimal() +
  theme(aspect.ratio = 1, legend.position = "none")

grid.arrange(plot_train, plot_test, ncol = 2)
```
```{r task6}
model <- tree(Colors ~ ., train_data)
draw.tree(model, cex=0.7)
```
```{r task6}
predict_tree <- predict(model, test_data[-3])

predicted_classes <- ifelse(predict_tree[, "red"] > predict_tree[, "green"], "red", "green")

conf_matrix <- table(test_data$Colors, predicted_classes)
accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)

cat("Accuracy:", accuracy, "\n")
```

# Задание 7

Разработать классификатор на основе дерева решений для данных Титаник (Titanic dataset) - https://www.kaggle.com/c/titanic
Исходные обучающие данные для классификации – в файле Titanic_train.csv
Данные для тестирования – в файле Titanic_test.csv

```{r titanic}
train_data <- read.csv("train.csv",stringsAsFactors = TRUE)
test_data <- read.csv("test.csv",stringsAsFactors = TRUE)
```

```{r titanic}
# Подготовка данных
preprocess_data <- function(data) {
  # Удаление столбцов
  columns_to_drop <- c("Name", "Ticket", "Cabin", "PassengerId")
  data <- data[, !(names(data) %in% columns_to_drop)]
  
  # Заполнение пропущенных значений
  data <- data %>%
    mutate(Age = ifelse(is.na(Age), mean(Age, na.rm = TRUE), Age),
           Fare = ifelse(is.na(Fare), mean(Fare, na.rm = TRUE), Fare),
           Embarked = ifelse(is.na(Embarked), levels(Embarked)[which.max(table(Embarked))], Embarked),
           Sex = ifelse(is.na(Sex), levels(Sex)[which.max(table(Sex))], Sex))
  
  # Преобразование категориальных переменных
  categorical_features <- c("Sex", "Embarked")
  
  for (feature in categorical_features) {
    data[[feature]] <- as.factor(data[[feature]])
  }
  return(data)
}

```

```{r titanic}
train_data_preprocessed <- preprocess_data(train_data)
train_data_preprocessed$Survived <- as.factor(train_data_preprocessed$Survived)
train_data_preprocessed
```
```{r titanic}
model <- tree(Survived ~., train_data_preprocessed)
draw.tree(model, cex=0.7)
```
```{r titanic}
test_data_preprocessed <- preprocess_data(test_data)
predict_tree <- predict(model, test_data_preprocessed)

predicted_classes <- ifelse(predict_tree[, '1'] > predict_tree[, '0'], 1, 0)
class_counts <- table(predicted_classes)

cat("Количество выживших: ", class_counts[1])
cat("\nКоличество погибших: ", class_counts[2])
```